{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубинное обучение для текстовых данных, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 1: Text Suggestion\n",
    "\n",
    "__Мягкий дедлайн: 24.09 23:59__   \n",
    "__Жесткий дедлайн: 27.09 23:59__\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит реализовать систему, предлагающую удачное продолжение слова или нескольких следующих слов в режиме реального времени по типу тех, которые используются в почте или поисковой строке. За дополнительные баллы полученную систему нужно будет обернуть в пользовательский интерфейс с помощью библиотеки [reflex](https://github.com/reflex-dev/reflex) или аналогов. В этой домашке вам не придется обучать никаких моделей, мы ограничимся n-граммной генерацией.\n",
    "\n",
    "### Структура\n",
    "\n",
    "Это домашнее задание состоит из двух частей: основной и бонусной. В первой вам нужно будет выполнить 5 заданий, по итогам которых вы получите минимально рабочее решение. А во второй, пользуясь тем, что вы уже сделали реализовать полноценную систему подсказки текста с пользовательским интерфейсом. Во второй части мы никак не будем ограничивать вашу фантазию. Делайте что угодно, лишь бы в результате получился удобный фреймворк. Чем лучше у вас будет результат, тем больше баллов вы получите. Если будет совсем хорошо, то мы добавим бонусов сверху по своему усмотрению.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — 15 баллов. Сдавать задание после жесткого дедлайна нельзя. При сдачи решения после мягкого дедлайна за каждый день просрочки снимается по __одному__ баллу.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Весь код должен быть написан самостоятельно. Чужим кодом для пользоваться запрещается даже с указанием ссылки на источник. В разумных рамках, конечно. Взять пару очевидных строчек кода для реализации какого-то небольшого функционала можно.\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код.\n",
    "\n",
    "При сдаче зададания в anytask вам будет необходимо сдать весь код, а если вы возьметесь за бонусную часть, то еще отчет и видео с демонстрацией вашего UI. За основную часть можно получить до __10-ти__ баллов, а за бонусную – до __5-ти__ баллов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "Для получения текстовых статистик используйте датасет `emails.csv`. Вы можете найти его по [ссылке](https://disk.yandex.ru/d/ikyUhWPlvfXxCg). Он содержит более 500 тысяч электронных писем на английском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517401"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emails = pd.read_csv('emails.csv')\n",
    "len(emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметьте, что данные очень грязные. В каждом письме содержится различная мета-информация, которая будет только мешать при предсказании продолжения текста.\n",
    "\n",
    "__Задание 1 (2 балла).__ Очистите корпус текстов по вашему усмотрению и объясните свой выбор. В идеале обработанные тексты должны содержать только текст самого письма и ничего лишнего по типу ссылок, адресатов и прочих символов, которыми мы точно не хотим продолжать текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import re\n",
    "def parse(text):\n",
    "    text = email.message_from_string(text).get_payload() # убираем хеадеры\n",
    "\n",
    "    pattern = re.compile(\n",
    "    r\"-----\\s*Original Message\\s*-----[\\s\\S]*?(?=\\n\\n|\\Z)\", \n",
    "    re.IGNORECASE\n",
    "    ) \n",
    "    text = re.sub(pattern, \"\", text) # убираем original message вставки\n",
    "\n",
    "    text = re.sub(r\"\\n([_*=\\-~])\\1{4,}[\\s\\S]*$\", \"\", text).strip() # убираем всякие шняги внизу аля подписи\n",
    "\n",
    "    text = re.sub(\n",
    "        r\"(?:^|\\n)(?:\\s*(From|To|Cc|cc|Subject):.*\\n?)+\", \n",
    "        \"\\n\", \n",
    "        text, \n",
    "        flags=re.IGNORECASE\n",
    "    ).strip() # убираем контакты кто-куда\n",
    "    text = re.sub(r\"http[s]?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text).strip() # убираем лишние переносы строк\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбор объясняю тем, чтобы оставить только само содержание без метанформации и подписей (причем сделать парсер максимально простым, чтобы быстро работал). Вообще в продакшн сетапе я возможно бы запаралися и использовал легковесную LLM, чтобы она спарсила при помощи хорошего систем промпта, так как регулярками все равно не покроешь все случаи. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message-ID: <21013688.1075844564560.JavaMail.evans@thyme>\n",
      "Date: Tue, 29 Aug 2000 01:26:00 -0700 (PDT)\n",
      "From: sara.shackleton@enron.com\n",
      "To: william.bradford@enron.com\n",
      "Subject: Re: Credit Derivatives\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: Sara Shackleton\n",
      "X-To: William S Bradford\n",
      "X-cc: \n",
      "X-bcc: \n",
      "X-Folder: \\Sara_Shackleton_Dec2000_June2001_1\\\n",
      "AFTER\n",
      "Bill:  Thanks for the info.   I also spoke with Jeff about how \n",
      "EnronCredit.com Ltd. was going to work since Dennis O'Connell (London lawyer) \n",
      "is responsible for that group.  Maybe you will be able to clarify which of \n",
      "Jeff's \"positions\" will be hedges and which will be backed to EnronCredit.  \n",
      "Maybe Rod will be handling most of Jeff's credit.  I'd appreciate an update.  \n",
      "Sara\n",
      "\tWilliam S Bradford\n",
      "\n",
      "==================================================\n",
      "Message-ID: <22688499.1075854130303.JavaMail.evans@thyme>\n",
      "Date: Mon, 24 Apr 2000 05:43:00 -0700 (PDT)\n",
      "From: pat.clynes@enron.com\n",
      "To: aimee.lannou@enron.com\n",
      "Subject: Meter #1591 Lamay Gaslift\n",
      "Cc: daren.farmer@enron.com\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "Bcc: daren.farmer@enron.com\n",
      "X-From: Pat Clynes\n",
      "X-To: Aimee Lannou\n",
      "X-cc: Daren J Farmer\n",
      "X-\n",
      "AFTER\n",
      "Aimee,\n",
      "Please check meter #1591 Lamay gas lift.  It doesn't appear to have very much \n",
      "flow and the\n",
      "BAV is showing the nom volume.  This could be adversely affecting the risk \n",
      "numbers.  Pat\n",
      "==================================================\n",
      "Message-ID: <27817771.1075841359502.JavaMail.evans@thyme>\n",
      "Date: Thu, 2 May 2002 04:54:27 -0700 (PDT)\n",
      "From: knipe3@msn.com\n",
      "To: fenner.chet@enron.com, joe.parks@enron.com, constantine.brian@enron.com, \n",
      "\twollam.erik@enron.com, corrier.brad@enron.com\n",
      "Subject: Re: man night again?\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "X-From: chad knipe <knipe3@msn\n",
      "AFTER\n",
      "GCCA Crawfish and rip-off raffle & over-priced print extravaganza tonight at approx\n",
      "6-6:30pm.  Pig and I and ?? and ?? will be there to be overserved and underfed.\n",
      "I've got my checkbook and wobbly boot ready to go!!\n",
      " \n",
      "C.\n",
      " \n",
      "what are you guys talking about? \n",
      "Screw it, I'm going to Tony's.\n",
      "what\n",
      " \n",
      "sh\"IN\" - dig.......get it?  aaaaaaaaaaaaaaaahhhhhhhhhhhhhaaaaaaaaaaaaahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhaaaaa\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for x in emails['message'].sample(3, random_state=42):\n",
    "    print(x[:400])\n",
    "    print(\"AFTER\")\n",
    "    print(parse(x)[:400])\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails['processed'] = emails['message'].apply(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill:  Thanks for the info.   I also spoke with Jeff about how \n",
      "EnronCredit.com Ltd. was going to work since Dennis O'Connell (London lawyer) \n",
      "is responsible for that group.  Maybe you will be able to clarify which of \n",
      "Jeff's \"positions\" will be hedges and which will be backed to EnronCredit.  \n",
      "Maybe Rod will be handling most of Jeff's credit.  I'd appreciate an update.  \n",
      "Sara\n",
      "\tWilliam S Bradford\n",
      "\t08/29/2000 07:24 AM\n",
      "Nelson/LON/ECT@ECT\n",
      "Sara,\n",
      "Please contact either Paul Radous or me on credit derivatives in the U.S.  \n",
      "Rod Nelson is the lead credit support for EnronCredit.com and should also be \n",
      "available in London, if necessary.  I am not aware of these recent trades but \n",
      "I am having lunch with Jeff Kinneman on Thursday to discuss among other \n",
      "things Credit support for his business.  \n",
      "It does concern me that we would offer to provide collateral DLJ without \n",
      "Treasury's approval.\n",
      "Bill\n",
      "I am seeing more and more credit derivatives.  The trades originating in \n",
      "Houston are coming from Jeff Kinneman's bond/debt traders but are booked in \n",
      "the name of ECT Investments, Inc. or ENA.  I can't be certain if these are \n",
      "backed to EnronCredit.com Limited.  \n",
      "In particular, there is a proposed total return bond trade with DLJ \n",
      "International Capital requiring ECT Investments to post collateral.  We have \n",
      "no master with this party.  Who in Houston credit  is looking at credit \n",
      "derivatives?\n",
      "FYI, other credit deals seem to be transacted wtih Deutsche Bank and UBS AG.\n",
      "Aimee,\n",
      "Please check meter #1591 Lamay gas lift.  It doesn't appear to have very much \n",
      "flow and the\n",
      "BAV is showing the nom volume.  This could be adversely affecting the risk \n",
      "numbers.  Pat\n",
      "GCCA Crawfish and rip-off raffle & over-priced print extravaganza tonight at approx\n",
      "6-6:30pm.  Pig and I and ?? and ?? will be there to be overserved and underfed.\n",
      "I've got my checkbook and wobbly boot ready to go!!\n",
      " \n",
      "C.\n",
      " \n",
      "what are you guys talking about? \n",
      "Screw it, I'm going to Tony's.\n",
      "what\n",
      " \n",
      "sh\"IN\" - dig.......get it?  aaaaaaaaaaaaaaaahhhhhhhhhhhhhaaaaaaaaaaaaahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhaaaaaaaaaaaaaaaaaaaahhhhhhhhhhhh\n",
      " \n",
      "in.\n",
      " \n",
      "BC\n",
      "Zero response from my last effort on this matter. Are your emails working \n",
      "men?\n",
      "Zander called and suggested we attend the CCA crawfish boil function to be \n",
      "held Thurday May 2, 2002. Reasonably priced event, beer, mud bugs and we can \n",
      "sit back and watch pig win raffle prizes.\n",
      "I'm in\n",
      "e.\n",
      "<<Keoni.zip>> Chris, per your request here are the 480 and 1480 charges\n",
      "that have been tracked to date for Enron.  Please keep in mind that the\n",
      "systems have not been built to incorporate this data into our Settlements\n",
      "process so the normal validations built into our Settlements process are not\n",
      "applied to the manual tracking of these charges. Therefore, this data should\n",
      "not be considered Settlement quality.  If these charges ever to become\n",
      "effective the proper systems will be built to pull in this data into the\n",
      "Settlement process. \n",
      "I checked with some folks and found out that FERC should rule on this during\n",
      "September. \n",
      "Take care. \n",
      "Keoni Almeida \n",
      "California Independent System Operator \n",
      "phone: 916/608-7053\n",
      "pager:  916/814-7352 \n",
      "alpha page:  9169812000.1151268@pagenet.net\n",
      "e-mail:  <mailto:kalmeida@caiso.com>\n",
      "I'm trying to change the Receipt Meter on deal 235367 from meter 7126 to \n",
      "meter 010902.  I'm not trying to change the Receipt Interconnect point, I'm \n",
      "trying to change the Receipt Point (From).  I can't change it.  Whats up?\n",
      "What if we replace Section 2 with something like \"This Agreement shall \n",
      "continue in effect unless and until terminated pursuant to Section 3,\"  and \n",
      "replace Section 3 with \"Either Party may terminate this Agreement upon \n",
      "written notice to the other Party not less than twelve (12) months prior to \n",
      "the intended date of termination.\"\n",
      "Sound OK?  if so I'll add it to my draft.\n",
      "David Foti\n",
      "05/10/2000 06:20 PM\n",
      "Looks like you nailed the least cost billing provision pretty good.  Just \n",
      "need to abrogate our remaining term and make all agreements evergreen.\n",
      "P.S. -- Dave, on your wish list you had included an evergreen provision.  See \n",
      "Section 2 of the original contract, which gives TW the right to extend for \n",
      "another five-year term by giving notice to SPS.  If TW does not give such \n",
      "notice, SPS may cancel the contract, and if it doesn't, the contract \n",
      "automatically rolls over for another year.  If this is not acceptable please \n",
      "let me know.\n",
      "---------------------- Forwarded by Phillip M Love/HOU/ECT on 06/07/2000 \n",
      "06:26 PM ---------------------------\n",
      "Jim Little\n",
      "05/12/2000 08:55 AM\n",
      "Nelson/HOU/ECT@ECT, Phillip M Love/HOU/ECT@ECT, Carrie Hollomon/HOU/ECT@ECT, \n",
      "Kellie Bilski/HOU/ECT@ECT, Rhonda Robinson/HOU/ECT@ECT, Rita \n",
      "Wynne/HOU/ECT@ECT, Gregory A Mausser/HOU/ECT@ECT, Janie Aguayo/HOU/ECT@ECT, \n",
      "Bryan Hull/HOU/ECT@ECT, John Valdes/HOU/ECT@ECT, Melba A Bowen/HOU/ECT@ECT, \n",
      "Cathy Sprowls/HOU/ECT@ECT\n",
      "Dear Mark,\n",
      "As per our discussion at the law conference and my voice mail to you earlier \n",
      "this week,  attached is Leona Tan's resume.  Please let me know if I can \n",
      "assist you further in this regard.  All the best!\n",
      "Cynthia\n",
      "got your message last night.  What is up?  Bet you are glad you are not working for the big E.  Things are not too good here.  Just waiting for the ax to drop.  Hope things are good in SF. Talk to you soon.\n",
      "PL\n",
      " \n",
      "Hey Philip!\n",
      "What's up? I have mixed feeling about you going to\n",
      "ND...I', jealous! This year is out for me\n",
      "unfortunately.  I have some friends that have gone to\n",
      "games this year...I'll get the skinny on scalping.  It\n",
      "shouldn't be much of a prop.  However, despite there\n",
      "records, it's still ND and Tennesse. It will be sold\n",
      "out.  But you can get tix.  I know this much. You\n",
      "can't (or it's against school policy to )get tix on\n",
      "campus. However, there are plenty of scalper on roads\n",
      "leading to campus.\n",
      "Sites to see.  Definietly check out the campus.  Make\n",
      "time to see the lakes, go by the dome etc. It's\n",
      "awesome. Also, it may be one of the games of the\n",
      "week...so, look out for espn, nbc and all the rest.\n",
      "The hype is great when networks other than nbc show.\n",
      "Lots of small bars...avoid the linebacker, unless you\n",
      "want to test that you have the endurance you had in\n",
      "undergrad. College Football Hall of Fame is\n",
      "there...downtown.  Tailgating is great....like I said\n",
      "make sure you get to the game early. That way you'll\n",
      "get the whole experience. Lots of tradition game day,\n",
      "for example the Band marches from the dome to the\n",
      "stadium....it's awesome.\n",
      "I'll give you a call at work..\n",
      "Later,\n",
      "Steve\n",
      "--- \"Love, Phillip M.\" <Phillip.M.Love@ENRON.com>\n",
      "wrote:\n",
      "> How are things going?  Are you still in San\n",
      "> Francisco?  If so I bet the\n",
      "> city has been going crazy about Barry Bonds and you\n",
      "> have been catching\n",
      "> flack about the wimpy Astros and how they pitched\n",
      "> him.\n",
      ">\n",
      "> Some guys and I make a trip every year to a\n",
      "> different college football\n",
      "> game and we changed it at the last minute this year.\n",
      ">  We are now going\n",
      "> to go to the Notre Dame v. Tennessee game.  I was\n",
      "> going to hit you up\n",
      "> for some information on your old stomping grounds.\n",
      "> Were and when is the\n",
      "> best place to scalp some tickets.  The brokers in\n",
      "> Chicago think way to\n",
      "> highly of a couple of teams with two or more losses.\n",
      ">  Where should we\n",
      "> hang out/ tailgate?  And what sites should we see\n",
      "> besides Touchdown\n",
      "> Jesus?\n",
      ">\n",
      "> Hope things are going well and give me a call when\n",
      "> you head back this\n",
      "> way.  Thanks.\n",
      "> PL\n",
      ">\n",
      ">\n",
      ">\n",
      ">\n",
      "Hello Darron,\n",
      "Just wanted to let you know that the following items from your\n",
      "order #11695228 have shipped and are on the way to your door:\n",
      "item sku 40157515    qty :1  STAR WARS 1-PH(S)DVD\n",
      "*This item shipped on 10/12/2001\n",
      "If you have any questions regarding this or any other order, please\n",
      "feel free to contact us at any time by visiting customer support at\n",
      "We built buy.com with one thing in mind - enabling you and all our\n",
      "customers to buy products better - that means offering you top brands,\n",
      "superstore selection, low prices, and outstanding service.\n",
      "Once again, thank you for your order. We look forward to earning your\n",
      "business as we set out to be the \"best place to buy on the Internet.\"\n",
      "Sincerely,\n",
      "Krista Smith\n",
      "Director, Customer Support\n",
      "* Please use the link below for your Customer Support questions. Please\n",
      "  do not reply to the buy.com email address.  For anytime Help click:\n"
     ]
    }
   ],
   "source": [
    "for x in emails['processed'].sample(10, random_state=42):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для следующего задания вам нужно будет токенизировать текст. Для этого просто разбейте его по словам. Очевидно, итоговый результат для финального пользователя будет лучше, если ваша система также будет предлагать уместную пунктуацию. Но если вы заметите, что из-за этого падает качество самого текса, то можете удалить все небуквенные символы на этапе токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str, clean_non_letters: bool = False, lowercase: bool = True) -> List[str]:\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    if not clean_non_letters:\n",
    "        return [t for t in text.split() if t]\n",
    "\n",
    "    return re.findall(r\"[a-zA-Zа-яА-ЯёЁ]+\", text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общая схема решения\n",
    "\n",
    "Мы хотим сделать систему, которая будет ускорять набор текста, советуя подходящие продолжения. Для подсказки следующего слова мы будем использовать n-граммную модель. Так как n-граммная модель работает с целыми словами, а советы мы хотим давать в риал-тайме даже когда слово еще не дописано, сперва надо научиться дополнять слово до целого."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнение слова\n",
    "\n",
    "В этой части вам предстоит реализовать метод дополнения слова до целого по его началу (префиксу). Для этого сперва необходимо научиться находить все слова, имеющие определенный префикс. Мы будем вызывать функцию поиска подходящих слов после каждой напечатанной пользователем буквы. Поэтому нам очень важно, чтобы поиск работал как можно быстрее. Простой перебор всех слов занимает $O(|V| \\cdot n)$ времени, где $|V|$ – размер словаря, а $n$ – длина префикса. Мы же напишем [префиксное дерево](https://ru.wikipedia.org/wiki/Префиксное_дерево), которое позволяет искать слова не больше чем за $O(n + mk)$, где $m$ - число подходящих слов, а $k$ – длина суффикса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2 (2 балла).__ Допишите префиксное дерево для поиска слов по префиксу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class PrefixTreeNode:\n",
    "    def __init__(self):\n",
    "        self.children: dict[str, PrefixTreeNode] = {}\n",
    "        self.is_end_of_word = False\n",
    "\n",
    "class PrefixTree:\n",
    "    def __init__(self, vocabulary: List[str]):\n",
    "        \"\"\"\n",
    "        vocabulary: список всех уникальных токенов в корпусе\n",
    "        \"\"\"\n",
    "        self.root = PrefixTreeNode()\n",
    "        \n",
    "        for word in vocabulary:\n",
    "            self.insert(word)\n",
    "\n",
    "    def insert(self, word: str) -> None:\n",
    "        node = self.root\n",
    "        for c in word:\n",
    "            if c not in node.children:\n",
    "                node.children[c] = PrefixTreeNode()\n",
    "            node = node.children[c]\n",
    "        node.is_end_of_word = True\n",
    "\n",
    "\n",
    "    def search_prefix(self, prefix) -> List[str]:\n",
    "        \"\"\"\n",
    "        Возвращает все слова, начинающиеся на prefix\n",
    "        prefix: str – префикс слова\n",
    "        \"\"\"\n",
    "\n",
    "        node = self.root\n",
    "        for c in prefix:\n",
    "            if c not in node.children:\n",
    "                return []\n",
    "            node = node.children[c]\n",
    "\n",
    "        result = []\n",
    "        self.dfs(node, prefix, result)\n",
    "        return result\n",
    "    \n",
    "    def dfs(self, node: PrefixTreeNode, prefix: str, result: List[str]) -> None:\n",
    "        if node.is_end_of_word:\n",
    "            result.append(prefix)\n",
    "        for c, child in node.children.items():\n",
    "            self.dfs(child, prefix + c, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ['aa', 'aaa', 'abb', 'bba', 'bbb', 'bcd']\n",
    "prefix_tree = PrefixTree(vocabulary)\n",
    "\n",
    "assert set(prefix_tree.search_prefix('a')) == set(['aa', 'aaa', 'abb'])\n",
    "assert set(prefix_tree.search_prefix('bb')) == set(['bba', 'bbb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда у нас есть способ быстро находить все слова с определенным префиксом, нам нужно их упорядочить по вероятности, чтобы выбирать лучшее. Будем оценивать вероятность слова по частоте его __встречаемости в корпусе__.\n",
    "\n",
    "__Задание 3 (2 балла).__ Допишите класс `WordCompletor`, который формирует словарь и префиксное дерево, а так же умеет находить все возможные продолжения слова вместе с их вероятностями. В этом классе вы можете при необходимости дополнительно отфильтровать слова, например, удалив все самые редкие. Постарайтесь максимально оптимизировать ваш код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "class WordCompletor:\n",
    "    def __init__(self, corpus):\n",
    "        \"\"\"\n",
    "        corpus: list – корпус текстов\n",
    "        \"\"\"\n",
    "        flat_corpus = [word for doc in corpus for word in doc]\n",
    "        self.word_counts = Counter(flat_corpus)\n",
    "        self.total_count = sum(self.word_counts.values())\n",
    "        vocabulary = list(self.word_counts.keys())\n",
    "        self.total_count = sum(self.word_counts.values())\n",
    "        self.prefix_tree = PrefixTree(vocabulary)\n",
    "\n",
    "    def get_words_and_probs(self, prefix: str) -> Tuple[List[str], List[float]]:\n",
    "        \"\"\"\n",
    "        Возвращает список слов, начинающихся на prefix,\n",
    "        с их вероятностями (нормировать ничего не нужно)\n",
    "        \"\"\"\n",
    "        words = self.prefix_tree.search_prefix(prefix)\n",
    "        probs = [self.word_counts[w] / self.total_count for w in words]\n",
    "        return words, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    [\"aa\", \"ab\"],\n",
    "    [\"aaa\", \"abab\"],\n",
    "    [\"abb\", \"aa\", \"ab\", \"bba\", \"bbb\", \"bcd\"],\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "words, probs = word_completor.get_words_and_probs('a')\n",
    "words_probs = list(zip(words, probs))\n",
    "assert set(words_probs) == {('aa', 0.2), ('ab', 0.2), ('aaa', 0.1), ('abab', 0.1), ('abb', 0.1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание следующих слов\n",
    "\n",
    "Теперь, когда мы умеем дописывать слово за пользователем, мы можем пойти дальше и предожить ему следующее слово (или несколько) с учетом дописанного. Для этого мы воспользуемся n-граммной моделью.\n",
    "\n",
    "Напомним, что вероятность последовательности для такой модели записывается по формуле\n",
    "$$\n",
    "P(w_1, \\dots, w_T) = \\prod_{i=1}^T P(w_i \\mid w_{i-1}, \\dots, w_{i-n}).\n",
    "$$\n",
    "\n",
    "$P(w_i \\mid w_{i-1}, \\dots, w_{i-n})$ оценивается по частоте встречаемости n-граммы.   \n",
    "\n",
    "__Задание 4 (2 балла).__ Напишите класс для n-граммной модели. Никакого сглаживания добавлять не надо, мы же не хотим, чтобы модель советовала случайные слова (хоть и очень редко)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        self.model = defaultdict(Counter)\n",
    "        for sent in corpus:\n",
    "            for i in range(len(sent) - n):\n",
    "                context = tuple(sent[i:i+n])\n",
    "                next_word = sent[i+n]\n",
    "                self.model[context][next_word] += 1\n",
    "\n",
    "    def get_next_words_and_probs(self, prefix):\n",
    "        if len(prefix) < self.n:\n",
    "            return [], []\n",
    "        context = tuple(prefix[-self.n:])\n",
    "        counter = self.model.get(context)\n",
    "        if not counter:\n",
    "            return [], []\n",
    "        total = sum(counter.values())\n",
    "        words, probs = [], []\n",
    "        for w, c in counter.items():\n",
    "            words.append(w)\n",
    "            probs.append(c / total)\n",
    "        return words, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "\n",
    "next_words, probs = n_gram_model.get_next_words_and_probs(['aa', 'aa'])\n",
    "words_probs = list(zip(next_words, probs))\n",
    "\n",
    "assert set(words_probs) == {('aa', 2/3), ('ab', 1/3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, мы теперь можем объединить два метода в автоматический дописыватель текстов: первый будет дополнять слово, а второй – предлагать продолжения. Хочется, чтобы предлагался список возможных продолжений, из который пользователь сможет выбрать наиболее подходящее. Самое сложное тут – аккуратно выбирать, что показывать, а что нет.   \n",
    "\n",
    "__Задание 5 (2 балла).__ В качестве первого подхода к снаряду реализуйте метод, возвращающий всегда самое вероятное продолжение жадным способом. После этого можно добавить опцию генерации нескольких вариантов продолжений, что сделает метод гораздо лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "import random\n",
    "class TextSuggestion:\n",
    "    def __init__(self, word_completor, n_gram_model):\n",
    "        self.word_completor = word_completor\n",
    "        self.n_gram_model = n_gram_model\n",
    "\n",
    "    def suggest_text(self, text: Union[str, list], n_words=3, n_texts=1) -> list[list[str]]:\n",
    "        \"\"\"\n",
    "        Возвращает возможные варианты продолжения текста (по умолчанию только один)\n",
    "        \n",
    "        text: строка или список слов – написанный пользователем текст\n",
    "        n_words: число слов, которые дописывает n-граммная модель\n",
    "        n_texts: число возвращаемых продолжений (пока что только одно)\n",
    "        \n",
    "        return: list[list[srt]] – список из n_texts списков слов, по 1 + n_words слов в каждом\n",
    "        Первое слово – это то, которое WordCompletor дополнил до целого.\n",
    "        \"\"\"\n",
    "\n",
    "        suggestions = []\n",
    "\n",
    "        tokens = text.split() if isinstance(text, str) else list(text)\n",
    "        if not tokens:\n",
    "            return []\n",
    "\n",
    "        last_word = tokens[-1]\n",
    "        completions, comp_probs = self.word_completor.get_words_and_probs(last_word)\n",
    "        if not completions:\n",
    "            return []  \n",
    "        completed_word = completions[0]\n",
    "\n",
    "        base_prefix = tokens[:-1] + [completed_word]\n",
    "\n",
    "        suggestions: List[List[str]] = [[] for _ in range(n_texts)]\n",
    "\n",
    "        for j in range(n_texts):\n",
    "            current_text = list(base_prefix)\n",
    "            suggestions[j].append(completed_word)\n",
    "\n",
    "            for _ in range(n_words):\n",
    "                next_words, probs = self.n_gram_model.get_next_words_and_probs(current_text)\n",
    "                if not next_words:\n",
    "                    break\n",
    "\n",
    "                if n_texts == 1:\n",
    "                    best_idx = max(range(len(next_words)), key=lambda i: probs[i])\n",
    "                    next_word = next_words[best_idx]\n",
    "                else:\n",
    "                    next_word = random.choices(next_words, weights=probs, k=1)[0]\n",
    "\n",
    "                current_text.append(next_word)\n",
    "                suggestions[j].append(next_word)\n",
    "\n",
    "        return suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_corpus = [\n",
    "    ['aa', 'aa', 'aa', 'aa', 'ab'],\n",
    "    ['aaa', 'abab'],\n",
    "    ['abb', 'aa', 'ab', 'bba', 'bbb', 'bcd']\n",
    "]\n",
    "\n",
    "word_completor = WordCompletor(dummy_corpus)\n",
    "n_gram_model = NGramLanguageModel(corpus=dummy_corpus, n=2)\n",
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)\n",
    "\n",
    "assert text_suggestion.suggest_text(['aa', 'aa'], n_words=3, n_texts=1) == [['aa', 'aa', 'aa', 'aa']]\n",
    "assert text_suggestion.suggest_text(['abb', 'aa', 'ab'], n_words=2, n_texts=1) == [['ab', 'bba', 'bbb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails['tokenized'] = emails['processed'].apply(tokenize, args=(True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails['tokenized_spaces'] = emails['processed'].apply(tokenize, args=(False, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 [here, is, our, forecast]\n",
       "1         [traveling, to, have, a, business, meeting, ta...\n",
       "2                           [test, successful, way, to, go]\n",
       "3         [randy, can, you, send, me, a, schedule, of, t...\n",
       "4                         [let, s, shoot, for, tuesday, at]\n",
       "                                ...                        \n",
       "517396    [this, is, a, trade, with, oil, spec, hedge, n...\n",
       "517397    [some, of, my, position, is, with, the, albert...\n",
       "517398    [morning, john, i, m, still, working, on, the,...\n",
       "517399    [analyst, rank, stephane, brodeur, chad, clark...\n",
       "517400    [i, think, the, ymca, has, a, class, that, is,...\n",
       "Name: tokenized, Length: 517401, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails['tokenized'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 [here, is, our, forecast]\n",
       "1         [traveling, to, have, a, business, meeting, ta...\n",
       "2                       [test, successful., way, to, go!!!]\n",
       "3         [randy,, can, you, send, me, a, schedule, of, ...\n",
       "4                  [let's, shoot, for, tuesday, at, 11:45.]\n",
       "                                ...                        \n",
       "517396    [this, is, a, trade, with, oil-spec-hedge-ng, ...\n",
       "517397    [some, of, my, position, is, with, the, albert...\n",
       "517398    [2, morning, john,, i'm, still, working, on, t...\n",
       "517399    [analyst, rank, stephane, brodeur, 1, chad, cl...\n",
       "517400    [i, think, the, ymca, has, a, class, that, is,...\n",
       "Name: tokenized_spaces, Length: 517401, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails['tokenized_spaces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_completor = WordCompletor(emails['tokenized'] )\n",
    "\n",
    "n_gram_model = NGramLanguageModel(corpus=emails['tokenized'] , n=1)\n",
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bro', 'confirm', 'that', 'the', 'company', 's']]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"hello bro\".split()\n",
    "text_suggestion.suggest_text(query, n_words=5, n_texts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_completor = WordCompletor(emails['tokenized_spaces'] )\n",
    "\n",
    "n_gram_model = NGramLanguageModel(corpus=emails['tokenized_spaces'] , n=1)\n",
    "text_suggestion = TextSuggestion(word_completor, n_gram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bro', 'confirm', 'that', 'the', 'following', 'the']]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"hello bro\".split()\n",
    "text_suggestion.suggest_text(query, n_words=5, n_texts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cringe', 'at', 'carolina.', 'tony'],\n",
       " ['cringe', 'out', 'of', 'any'],\n",
       " ['cringe', 'out', 'why!', 'vince']]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"cringe\".split()\n",
    "text_suggestion.suggest_text(query, n_words=3, n_texts=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть: Добавляем UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускать ячейки в юпитере – это хорошо, но будет лучше, если вашим решением действительно можно будет пользоваться. Для этого вам предлагается добавить полноценных User Interface. Мы рекомендуем использовать для этого [reflex](https://github.com/reflex-dev/reflex). Это Python библиотека для создания web-интерфейсом с очень богатым функционалом.\n",
    "\n",
    "Ваша задача – сделать поле для текстового ввода, при наборе текста в котором будут появляться подсказки в реальном времени. Продумайте, как пользователь будет выбирать подсказки, сколько продолжений рекомендавать и так далее. В общем, должно получиться красиво и удобно. В этой части вы можете модифицировать все классы по своему усмотрению и добавлять любые эвристики. Если нужно, то дополнительно обрабатывать текст и вообще делать все, что считаете нужным. \n",
    "\n",
    "За это задание можно получить до __5-ти бонусных баллов__ в зависимости о того, насколько хорошо и удобно у вас получилось. При сдаче задания прикрепите небольшой __отчет__ (полстраницы) с описанием вашей системы, а также __видео__ (1-2 минуты) с демонстрацией работы интерфейса.\n",
    "\n",
    "Мы настоятельно рекомендуем вам оформить код в проект, а не писать в ноутбуке. Но если вам очень хочется писать тут, то хотя бы не меняйте код в предыдущих заданиях, чтобы его можно было нормально оценивать."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HSE_NLP (3.10.16)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
  "notebookPath": "seminar.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
