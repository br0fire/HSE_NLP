{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ead9813-da8f-4114-add5-68498e1a7474",
   "metadata": {},
   "source": [
    "### Low-Rank Adaptation (LoRA)\n",
    "\n",
    "<img src=\"https://heidloff.net/assets/img/2023/08/lora.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "__Задание 1 (3 балла).__ Реализуйте самостоятельно модуль LoRA для эффективного обучения LLM по схеме, описанной в [статье](https://arxiv.org/pdf/2106.09685). Встройте его в свою любимую LLM и убедитесь, что ошибка убывает при обучении параметров LoRA на безусловную генерацию. Для этого возьмите любые данные на свой выбор. Замерьте насколько уменьшилось число обучаемых параметров, как изменилась скорость во время forward и backward процессов и как изменились затраты по памяти. Сделайте выводы и напишите о них в отчете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "681e3557-94dc-4194-9727-4d1c26a4df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from transformers.models.gpt2.modeling_gpt2 import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c29825e-1366-47fc-b935-6d1dec7dbe9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3bb5beb-a4a0-4e1a-aaab-780c15cfd04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"stas/openwebtext-10k\", split=\"train\", trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7960da8-b8a6-4789-961a-f75150975912",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "train_dataset = tokenized\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc5f4f1-1c04-4799-b66e-b69f72af9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAConv1D(nn.Module):\n",
    "    def __init__(self, base: Conv1D, r=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        in_f, out_f = base.weight.shape\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scale = alpha / r\n",
    "\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_f, r) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(r, out_f))\n",
    "\n",
    "        for p in base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "        x2 = x.view(-1, x.shape[-1])\n",
    "        lora = x2 @ self.lora_A @ self.lora_B\n",
    "        lora = lora.view(*x.shape[:-1], -1)\n",
    "        return out + self.scale * lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da5507f9-caff-460c-b2e4-5ffb33882344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lora(model, r=8, alpha=16):\n",
    "    for name, module in model.named_modules():\n",
    "        if name.endswith(\".attn\"):\n",
    "            if isinstance(module.c_attn, Conv1D):\n",
    "                module.c_attn = LoRAConv1D(module.c_attn, r, alpha)\n",
    "            if isinstance(module.c_proj, Conv1D):\n",
    "                module.c_proj = LoRAConv1D(module.c_proj, r, alpha)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        if \"lora_\" in n:\n",
    "            p.requires_grad = True\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a593966d-7364-4a0e-95e9-464e7b207d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params(model, title=\"\"):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"{title}TOTAL PARAMS: {total:,}\")\n",
    "    print(f\"{title}TRAINABLE PARAMS: {trainable:,}\")\n",
    "    print(f\"{title}PERCENT TRAINABLE: {100 * trainable / total:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e616452f-2b0d-418f-a014-5713e641b2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA] TOTAL PARAMS: 124,882,176\n",
      "[LoRA] TRAINABLE PARAMS: 442,368\n",
      "[LoRA] PERCENT TRAINABLE: 0.3542%\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "\n",
    "model = add_lora(model, r=8, alpha=16)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "print_params(model, \"[LoRA] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4bb7483-255f-471c-9fec-a176877889f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./lora_output\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=50,\n",
    "    save_steps=1000000,\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fa198c0-064f-4bb0-9866-e0804c617d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a1e285e-02b9-4e88-8e27-1ee564ad12be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 01:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.343900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.293900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.253500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.267100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=313, training_loss=3.303958746571891, metrics={'train_runtime': 66.283, 'train_samples_per_second': 150.868, 'train_steps_per_second': 4.722, 'total_flos': 1313254932480000.0, 'train_loss': 3.303958746571891, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b9339d6-d557-4872-9a4b-d3435349b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_speed_and_memory(model, tokenizer, collator, dataset, steps=30):\n",
    "    model.eval()\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    dl = DataLoader(dataset, batch_size=4, shuffle=False, collate_fn=collator)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    it = iter(dl)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(steps):\n",
    "        batch = next(it)\n",
    "\n",
    "        for k in batch:\n",
    "            batch[k] = batch[k].to(\"cuda\")\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        out = model(**batch)\n",
    "        loss = out.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "\n",
    "        times.append(end - start)\n",
    "\n",
    "    avg_step = sum(times) / len(times)\n",
    "    max_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "    print(f\"Average step time (forward+backward): {avg_step:.4f} sec\")\n",
    "    print(f\"Peak GPU memory: {max_mem:.1f} MB\")\n",
    "\n",
    "    return avg_step, max_mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "650a1fe4-792a-4c87-9297-df9af54800d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Measuring LoRA model performance ===\n",
      "Average step time (forward+backward): 0.0486 sec\n",
      "Peak GPU memory: 2137.6 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Measuring LoRA model performance ===\")\n",
    "avg_time, peak_mem = measure_speed_and_memory(model, tokenizer, collator, train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b24b3e1e-6a7e-4ac6-a2e9-9eb19f9a9535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 31571926-422a-405c-bece-dcdbddcb7c26)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full FT parameters ===\n",
      "[FULL] TOTAL PARAMS: 124,439,808\n",
      "[FULL] TRAINABLE PARAMS: 124,439,808\n",
      "[FULL] PERCENT TRAINABLE: 100.0000%\n",
      "=== Measuring FULL FT speed ===\n",
      "Average step time (forward+backward): 0.0629 sec\n",
      "Peak GPU memory: 2820.2 MB\n"
     ]
    }
   ],
   "source": [
    "model_full = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model_full.config.pad_token_id = tokenizer.pad_token_id\n",
    "model_full.config.use_cache = False\n",
    "\n",
    "# full finetuning — все параметры trainable\n",
    "for p in model_full.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "model_full.to(\"cuda\")\n",
    "\n",
    "print(\"=== Full FT parameters ===\")\n",
    "print_params(model_full, \"[FULL] \")\n",
    "\n",
    "print(\"=== Measuring FULL FT speed ===\")\n",
    "full_time, full_mem = measure_speed_and_memory(model_full, tokenizer, collator, train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d67b102-bddf-4b15-81f0-c7ef6606347f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA] TOTAL PARAMS: 124,882,176\n",
      "[LoRA] TRAINABLE PARAMS: 442,368\n",
      "[LoRA] PERCENT TRAINABLE: 0.3542%\n",
      "\n",
      "LoRA avg step time:   0.0486 sec\n",
      "LoRA peak memory:     2137.6 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_params(model, \"[LoRA] \")\n",
    "\n",
    "print(f\"\\nLoRA avg step time:   {avg_time:.4f} sec\")\n",
    "print(f\"LoRA peak memory:     {peak_mem:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f72f982-dbb9-440b-8e13-d7766c699b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full FT avg step time: 0.0629 sec\n",
      "Full FT peak memory:   2820.2 MB\n",
      "Speed improvement:     22.7% faster\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFull FT avg step time: {full_time:.4f} sec\")\n",
    "print(f\"Full FT peak memory:   {full_mem:.1f} MB\")\n",
    "\n",
    "reduction_ratio = 100 * (1 - avg_time/full_time)\n",
    "print(f\"Speed improvement:     {reduction_ratio:.1f}% faster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927974bd-4b00-4a8b-89b1-7189f13b8f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azaza",
   "language": "python",
   "name": "azaza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
